{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections import deque\n",
    "import math\n",
    "from datetime import datetime\n",
    "import findspark\n",
    "\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = \"1\"\n",
    "\n",
    "from functools import reduce as _reduce\n",
    "from toposort import toposort, CircularDependencyError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pyspark.sql import SparkSession\n",
    "import databricks.koalas as ks\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "\n",
    "import preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_reservation(manager, timeout, reservation_id, quiet=True):\n",
    "    pm = manager\n",
    "    starttime = time.time()\n",
    "    lasttime = starttime + int(timeout)\n",
    "\n",
    "    waittime = 5\n",
    "    timeswaited = 0\n",
    "\n",
    "    while True:\n",
    "        state = pm.fetch_reservation(reservation_id).state\n",
    "        if state == \"R\":\n",
    "            break\n",
    "\n",
    "        curtime = time.time()\n",
    "        maxwaittime = lasttime - curtime\n",
    "        nextwaittime = int(min(maxwaittime, waittime))\n",
    "        if nextwaittime <= 0:\n",
    "            print(\"[%.1f] Current state: %s. Reached timeout.\" % (curtime, state))\n",
    "            sys.exit(\"wait-for-reservation timed out\")\n",
    "        if not quiet:\n",
    "            print(\"[%.1f] Current state: %s. Waiting %u more seconds.\" % (curtime, state, nextwaittime))\n",
    "        time.sleep(nextwaittime)\n",
    "\n",
    "        timeswaited += 1\n",
    "        if timeswaited == 12:\n",
    "            waittime = 10 # After a minute, decrease the polling frequency\n",
    "        elif timeswaited == 36:\n",
    "            waittime = 15 # After 5 minutes, decrease the polling frequency\n",
    "        elif timeswaited == 76:\n",
    "            waittime = 30 # After 15 minutes, decrease the polling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookahead_newer(df) -> pd.DataFrame[\"workflow_id\": int, \"task_id\": int, \"task_slack\": int, \"minimal_start_time\": int]:\n",
    "    df.set_index(\"id\", inplace=True)\n",
    "    \n",
    "    df = df[((df['children'].map(len) > 0) | (df['parents'].map(len) > 0))]\n",
    "    \n",
    "    if(len(df) == 0):\n",
    "        return pd.DataFrame(columns=[\"workflow_id\", \"task_id\", \"task_slack\", \"minimal_start_time\"])\n",
    "  \n",
    "    graph = dict()\n",
    "    forward_dict = dict()\n",
    "    task_runtimes = dict()\n",
    "    task_arrival_times = dict()\n",
    "    workflow_id = None\n",
    "\n",
    "    for row in df.to_records(): # 0: task id, 1: wf id, 2: children, 3: parents, 4: ts submit, 5: runtime\n",
    "        graph[row[0]] = set(row[3].flatten())\n",
    "        forward_dict[row[0]] = set(row[2].flatten())\n",
    "        task_runtimes[row[0]] = row[5]\n",
    "        task_arrival_times[row[0]] = row[4]\n",
    "        workflow_id = row[1]\n",
    "        \n",
    "    del df\n",
    "    del row\n",
    "    \n",
    "    try:\n",
    "        groups = list(toposort(graph))\n",
    "    except CircularDependencyError:\n",
    "        del forward_dict\n",
    "        del task_runtimes\n",
    "        del task_arrival_times\n",
    "        del workflow_id\n",
    "        del graph\n",
    "        return pd.DataFrame(columns=[\"workflow_id\", \"task_id\", \"task_slack\", \"minimal_start_time\"])\n",
    "    \n",
    "    del graph\n",
    "    \n",
    "    for group in groups:\n",
    "        for task_id in group:\n",
    "            task_done = task_runtimes[task_id] + task_arrival_times[task_id]\n",
    "\n",
    "            for c in forward_dict[task_id]:\n",
    "                # Given the runtime of our parent, check if the submit time of this task is dominant or\n",
    "                # if our parent is dominant.\n",
    "                if c not in task_runtimes: continue  # Task was not in snapshot of trace\n",
    "                if task_done > task_arrival_times[c]:\n",
    "                    task_arrival_times[c] = task_done\n",
    "                \n",
    "\n",
    "    del task_done\n",
    "    del c\n",
    "\n",
    "    rows = deque() # More memory efficient, see https://towardsdatascience.com/memory-efficiency-of-common-python-data-structures-88f0f720421\n",
    "    for task_id in task_runtimes.keys():\n",
    "        min_child = None\n",
    "        for c in forward_dict[task_id]:\n",
    "            if c not in task_arrival_times: continue\n",
    "            if min_child is None:\n",
    "                min_child = task_arrival_times[c]\n",
    "            else:\n",
    "                min_child = min(task_arrival_times[c], min_child)\n",
    "        \n",
    "        # If it's an end task (with no childen) - just put its finish time there then.\n",
    "        if min_child is None:\n",
    "            rows.append([workflow_id, task_id, 0, task_arrival_times[task_id]])\n",
    "        elif min_child >= task_arrival_times[task_id] + task_runtimes[task_id]:\n",
    "            rows.append([workflow_id, task_id, min_child - (task_arrival_times[task_id] + task_runtimes[task_id]), task_arrival_times[task_id]])\n",
    "        else:\n",
    "            raise Exception(\"Error: A child should never have a start time lower than a parent\")\n",
    "    \n",
    "                        \n",
    "    del task_arrival_times\n",
    "    del task_runtimes\n",
    "    del forward_dict\n",
    "    del min_child\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"workflow_id\", \"task_id\", \"task_slack\", \"minimal_start_time\"])  # Remove [:0] to return the entire DF. Also remove to_parquet then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1617278042.0] Current state: PD. Waiting 5 more seconds.\n",
      "We are on DAS5, node053.ib.cluster is master.\n",
      "Loading data into HDFS\n",
      "Done! Starting...\n"
     ]
    }
   ],
   "source": [
    "location = \"./WTA/parquet\"\n",
    "if 'DAS5' in os.environ:  # If we want to execute it on the DAS-5 super computer\n",
    "    if 'DEPLOYER_HOME' not in os.environ:\n",
    "            print(\"NEED TO SET $DEPLOYER_HOME - see Tim Hegeman's DAS deploy script\")\n",
    "            exit(-2)\n",
    "            \n",
    "    os.environ['JAVA_HOME'] = \"/usr/lib/jvm/jre-11-openjdk\"\n",
    "    \n",
    "    hadoop_version = \"3.2.2\"\n",
    "    spark_version = \"3.1.1\"\n",
    "    \n",
    "    import pyspark\n",
    "    if str(pyspark.__version__) != spark_version:\n",
    "        print(str(pyspark.__version__), spark_version)\n",
    "        print(\"Version mismatch between spark and pyspark. Update one or downgrade the other.\")\n",
    "        exit(-1)\n",
    "    \n",
    "    num_machines = 11\n",
    "    reservation_manager = preserve.get_PreserveManager()\n",
    "    reservation_id = reservation_manager.create_reservation(num_machines, \"72:00:00\")\n",
    "    wait_for_reservation(reservation_manager, 12000, str(reservation_id), False)\n",
    "    \n",
    "    master_node = reservation_manager.get_own_reservations()[reservation_id].assigned_machines[0]\n",
    "    print(\"We are on DAS5, {0} is master.\".format(master_node))\n",
    "    \n",
    "    # Now start Hadoop and Spark\n",
    "    os.system(\"cd {}; ./deployer deploy --preserve-id {} -s env/das5-spark.settings spark {}\".format(os.environ['DEPLOYER_HOME'], reservation_id, spark_version))\n",
    "    os.system(\"cd {}; ./deployer deploy --preserve-id {} -s env/das5-hadoop.settings hadoop {} yarn_enable=false\".format(os.environ['DEPLOYER_HOME'], reservation_id, hadoop_version))\n",
    "    \n",
    "    try:\n",
    "        findspark.init(f'./big-data-frameworks/spark-{spark_version}') \n",
    "        spark = SparkSession.builder \\\n",
    "            .master(\"spark://\" + master_node + \":7077\") \\\n",
    "            .appName(\"Energy Efficiency Data Analysis\") \\\n",
    "            .config(\"spark.executor.memory\", \"60G\") \\\n",
    "            .config(\"spark.executor.cores\", \"16\") \\\n",
    "            .config(\"spark.executor.instances\", \"1\") \\\n",
    "            .config(\"spark.driver.memory\", \"60G\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "            .config(\"spark.local.dir\", \"/localspark, /tmp\") \\\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"128m\") \\\n",
    "            .config('spark.memory.storageFraction', 0.6) \\\n",
    "            .config('spark.memory.fraction', 0.4) \\\n",
    "            .config('spark.rdd.compress', 'true') \\\n",
    "            .config('spark.checkpoint.compress', 'true') \\\n",
    "            .config('spark.sql.shuffle.partitions', 4*(num_machines-1)*16) \\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        print(\"Spark could not be started!!\")\n",
    "        reservation_manager.kill_reservation(reservation_id)\n",
    "        raise e\n",
    "    \n",
    "    print(\"Loading data into HDFS\")\n",
    "    folders = next(os.walk(location))[1]\n",
    "    for folder in folders:\n",
    "        # Skip too large datasets or those without task dependencies\n",
    "        if \"alibaba\" in str(folder).lower() and \"100k\" not in str(folder).lower(): continue\n",
    "        if \"google\" in str(folder).lower(): continue\n",
    "        if \"lanl\" in str(folder).lower(): continue\n",
    "        if \"two_sigma\" in str(folder).lower(): continue\n",
    "\n",
    "        data_folder = os.path.join(location, folder, \"tasks\", \"schema-1.0\")\n",
    "        if not os.path.exists(data_folder): continue\n",
    "        hdfs_path = os.path.join(\"/WTA/parquet/\", folder, \"tasks\", \"schema-1.0\")\n",
    "        os.system(\"cd {}; ./frameworks/hadoop-{}/bin/hdfs dfs -mkdir -p {}\".format(os.environ['DEPLOYER_HOME'], hadoop_version, hdfs_path))\n",
    "        os.system(\"cd {}; ./frameworks/hadoop-{}/bin/hdfs dfs -copyFromLocal {} {}\".format(os.environ['DEPLOYER_HOME'], hadoop_version, os.path.join(data_folder, \"*\"), hdfs_path))\n",
    "    print(\"Done! Starting...\")\n",
    "else:\n",
    "    findspark.init(spark_home=\"<path to spark>\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[8]\") \\\n",
    "        .appName(\"WTA parser\") \\\n",
    "        .config(\"spark.executor.memory\", \"20G\") \\\n",
    "        .config(\"spark.driver.memory\", \"8G\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = \"./ic2e-wta-output\"\n",
    "hdfs_path = f\"hdfs://{master_node}:9000/WTA/parquet/\"\n",
    "try:\n",
    "    folders = next(os.walk(location))[1]\n",
    "    \n",
    "    for folder in folders:\n",
    "        if \"alibaba\" in str(folder).lower() and \"100k\" not in str(folder).lower(): continue\n",
    "        if \"google\" in str(folder).lower(): continue\n",
    "        if \"lanl\" in str(folder).lower(): continue\n",
    "        if \"two_sigma\" in str(folder).lower(): continue\n",
    "        data_folder = os.path.join(location, folder)\n",
    "        output_location_look_ahead = os.path.join(output_location, \"look_ahead\", folder.replace(\"_parquet\", \"\") + \"_slack.parquet\")\n",
    "\n",
    "        if not os.path.exists(os.path.join(data_folder, \"tasks\", \"schema-1.0\")): continue\n",
    "        \n",
    "        print(os.path.join(hdfs_path, folder, \"tasks\", \"schema-1.0\"))\n",
    "        try:\n",
    "            kdf = ks.read_parquet(os.path.join(hdfs_path, folder, \"tasks\", \"schema-1.0\"),\n",
    "                         columns=[\n",
    "                             \"workflow_id\", \"id\", \"task_id\", \"children\", \"parents\", \"ts_submit\", \"runtime\", \n",
    "                         ], pandas_metadata=False, engine='pyarrow')\n",
    "        except:\n",
    "            kdf = ks.read_parquet(os.path.join(hdfs_path, folder, \"tasks\", \"schema-1.0\"),\n",
    "                     columns=[\n",
    "                         \"workflow_id\", \"id\", \"task_id\", \"children\", \"parents\", \"ts_submit\", \"runtime\"\n",
    "                     ], pandas_metadata=True, engine='pyarrow')\n",
    "            \n",
    "        os.makedirs(output_location_look_ahead, exist_ok=True)\n",
    "        \n",
    "        if \"task_id\" in kdf.columns:\n",
    "            kdf = kdf.rename(columns={\"task_id\": \"id\"})\n",
    "            \n",
    "        print(\"Removing NAs\")\n",
    "        kdf.dropna(inplace=True)\n",
    "            \n",
    "        def parse_kdf(kdf, append=False):\n",
    "            print(\"Grouping based on workflow ID\")\n",
    "            grouped_df = kdf.groupby(\"workflow_id\")\n",
    "\n",
    "            print(\"Running look ahead\")\n",
    "            grouped_df.apply(lookahead_newer) \\\n",
    "                .to_parquet(output_location_look_ahead, compression='snappy', engine='pyarrow',\n",
    "                           mode='append' if append else 'overwrite')\n",
    "            \n",
    "        parse_kdf(kdf)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Exception\")\n",
    "    raise e\n",
    "finally:\n",
    "    print(\"Stopping...\")\n",
    "    spark.stop()\n",
    "    reservation_manager.kill_reservation(reservation_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
